{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    DataCollatorWithPadding, \n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForAudioClassification,\n",
    "    Wav2Vec2Processor,\n",
    "    utils,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np\n",
    "from datasets import DatasetDict\n",
    "import torch\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "utils.logging.set_verbosity_error()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "\n",
    "TER_MODEL_ID = \"xlm-roberta-base\"\n",
    "SER_MODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-russian\"\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "class SpeechTextEmotionClassifier:\n",
    "    def __init__(\n",
    "        self, \n",
    "        df_paths={'train': 'data/toloka_marked_train.csv', 'test': 'data/toloka_marked_test.csv'},\n",
    "        speech_paths={'train': 'data/speech_train.npz', 'test': 'data/speech_test.npz'},\n",
    "    ):\n",
    "        self.metric = load_metric(\"accuracy\")\n",
    "        \n",
    "        self.ter_model = AutoModelForSequenceClassification.from_pretrained(TER_MODEL_ID, num_labels=4)\n",
    "        \n",
    "        self.ser_model = AutoModelForAudioClassification.from_pretrained(SER_MODEL_ID, num_labels=4)\n",
    "        self.ser_model.freeze_feature_encoder()\n",
    "        \n",
    "        dataset = load_dataset(\n",
    "            'csv', \n",
    "            data_files=df_paths,\n",
    "        )\n",
    "\n",
    "        def add_speech(path, dataset):\n",
    "            with np.load(path) as data:\n",
    "                speech = [data[i] for i in data]\n",
    "\n",
    "            return dataset.add_column('speech', speech)\n",
    "        \n",
    "        for split, path in speech_paths.items():\n",
    "            dataset[split] = add_speech(path, dataset[split])\n",
    "        \n",
    "        test_split = dataset['test'].train_test_split(shuffle=True, seed=200, test_size=0.5)\n",
    "        self.dataset = DatasetDict({\n",
    "            'train': dataset['train'],\n",
    "            'validation': test_split['train'],\n",
    "            'test': test_split['test'],\n",
    "            })\n",
    "\n",
    "        \n",
    "        self.ser_processor = Wav2Vec2Processor.from_pretrained(SER_MODEL_ID)\n",
    "\n",
    "        self.ter_tokenizer = AutoTokenizer.from_pretrained(TER_MODEL_ID)\n",
    "        \n",
    "        self.combined_model = SVC(\n",
    "            C=1.0, \n",
    "            kernel='poly', \n",
    "            degree=5, \n",
    "            gamma='scale', \n",
    "            coef0=0.0, \n",
    "            probability=True, \n",
    "            tol=0.001, \n",
    "            max_iter=-1, \n",
    "            decision_function_shape='ovr',\n",
    "            class_weight=None,\n",
    "            random_state=12,\n",
    "        )\n",
    "        \n",
    "        self.ter_output_dir = \"ter_outputs\"\n",
    "        self.ser_output_dir = \"ser_outputs\"\n",
    "\n",
    "\n",
    "    def compute_metrics(self, eval_pred):\n",
    "        \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "        predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "        return self.metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "    \n",
    "    def preprocess_audio_df(self, batch):\n",
    "    #     batch[\"input_values\"] = self.ser_processor(batch[\"speech\"], sampling_rate=SAMPLE_RATE).input_values\n",
    "        batch[\"input_values\"] = batch[\"speech\"]\n",
    "        batch[\"labels\"] = batch[\"label\"]\n",
    "        return batch\n",
    "    \n",
    "    def construct_speech_df(self):\n",
    "        return self.dataset.map(\n",
    "            self.preprocess_audio_df, \n",
    "            remove_columns=['path', 'hypo', 'result', 'label', 'speech'], \n",
    "            batched=True\n",
    "        )\n",
    "    \n",
    "    def preprocess_text_df(self, batch):\n",
    "        batch[\"input_ids\"] = self.ter_tokenizer(batch[\"hypo\"]).input_ids\n",
    "        batch[\"labels\"] = batch[\"label\"]\n",
    "        return batch\n",
    "    \n",
    "    def construct_text_df(self):\n",
    "        return self.dataset.map(\n",
    "            self.preprocess_text_df, \n",
    "            remove_columns=['path', 'hypo', 'result', 'label', 'speech'], \n",
    "            batched=True,\n",
    "        )\n",
    "    \n",
    "    def create_ter_trainer(\n",
    "        self, \n",
    "        df, \n",
    "        lr=3e-4, \n",
    "        n_epochs=5, \n",
    "        tp16=True, \n",
    "        disable_tqdm=True,\n",
    "    ):\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.ter_output_dir,\n",
    "            group_by_length=True,\n",
    "            evaluation_strategy = \"epoch\",\n",
    "            logging_strategy = \"epoch\",\n",
    "            learning_rate=lr,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            gradient_accumulation_steps=64//batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=n_epochs,\n",
    "            warmup_ratio=0.1,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            fp16=tp16,\n",
    "            save_strategy = \"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            save_total_limit=2,\n",
    "            disable_tqdm=disable_tqdm,\n",
    "        )\n",
    "        \n",
    "        return Trainer(\n",
    "            model=self.ter_model,\n",
    "            args=training_args,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "            train_dataset=df['train'],\n",
    "            eval_dataset=df['validation'],\n",
    "            tokenizer=self.ter_tokenizer,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def train_model(\n",
    "        self, \n",
    "        trainer,\n",
    "        save_model=False,\n",
    "    ):  \n",
    "        trainer.train()\n",
    "        if save_model:\n",
    "            trainer.save_model(self.ter_output_dir)\n",
    "        \n",
    "    def create_ser_trainer(\n",
    "        self, \n",
    "        df, \n",
    "        lr=3e-4, \n",
    "        n_epochs=5, \n",
    "        tp16=True, \n",
    "        disable_tqdm=True,\n",
    "    ):\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.ser_output_dir,\n",
    "            group_by_length=True,\n",
    "            evaluation_strategy = \"epoch\",\n",
    "            logging_strategy = \"epoch\",\n",
    "            learning_rate=lr,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            gradient_accumulation_steps=64//batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=n_epochs,\n",
    "            warmup_ratio=0.1,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            fp16=tp16,\n",
    "            save_strategy = \"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            save_total_limit=2,\n",
    "            disable_tqdm=disable_tqdm,\n",
    "        )\n",
    "        \n",
    "        return Trainer(\n",
    "            model=self.ser_model,\n",
    "            args=training_args,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "            train_dataset=df['train'],\n",
    "            eval_dataset=df['validation'],\n",
    "            tokenizer=self.ser_processor.feature_extractor,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def load_pretrained_model(\n",
    "        self, \n",
    "        model_type, \n",
    "        df,\n",
    "        model_dir=None,\n",
    "        lr=3e-4, \n",
    "        n_epochs=5, \n",
    "        tp16=True, \n",
    "        disable_tqdm=True,\n",
    "    ):\n",
    "        if model_type == 'ter':\n",
    "            self.ter_model = AutoModelForSequenceClassification.from_pretrained(self.ter_output_dir if not model_dir else model_dir)\n",
    "            trainer = self.create_ter_trainer(df, lr, n_epochs, tp16, disable_tqdm)\n",
    "        elif model_type == 'ser':\n",
    "            self.ser_model = AutoModelForAudioClassification.from_pretrained(self.ser_output_dir if not model_dir else model_dir)\n",
    "            trainer = self.create_ser_trainer(df, lr, n_epochs, tp16, disable_tqdm)\n",
    "        return trainer\n",
    "        \n",
    "        \n",
    "    def predict_text_model(self, text_trainer, df):\n",
    "        return text_trainer.predict(df).predictions\n",
    "    \n",
    "    def predict_speech_model(self, speech_trainer, df):\n",
    "        return speech_trainer.predict(df).predictions\n",
    "    \n",
    "    def predict_and_concat(self, text_trainer, speech_trainer, text_df, speech_df):\n",
    "        text_logits = self.predict_text_model(text_trainer, text_df)\n",
    "        speech_logits = self.predict_speech_model(speech_trainer, speech_df)\n",
    "        \n",
    "        return np.concatenate((text_logits, speech_logits), axis=1)\n",
    "    \n",
    "    def train_combined(self, text_trainer, speech_trainer, text_df, speech_df):\n",
    "        logits = self.predict_and_concat(text_trainer, speech_trainer, text_df, speech_df)\n",
    "        \n",
    "        self.combined_model.fit(logits, self.dataset['train']['label'])\n",
    "        \n",
    "    def predict_combined(self, text_trainer, speech_trainer, text_df, speech_df):\n",
    "        logits = self.predict_and_concat(text_trainer, speech_trainer, text_df, speech_df)\n",
    "        return self.combined_model.predict_proba(logits)\n",
    "        \n",
    "        \n",
    "    def eval_metrics(self, text_trainer, speech_trainer, text_df, speech_df):\n",
    "        logits = self.predict_combined(text_trainer, speech_trainer,text_df, speech_df)\n",
    "        predictions = np.argmax(logits, axis=1)\n",
    "        self.metric.add_batch(predictions=predictions, references=text_df[\"labels\"])\n",
    "\n",
    "        print(self.metric.compute())\n",
    "\n",
    "        cm = confusion_matrix(text_df[\"labels\"], predictions)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.title('Матрица ошибок')\n",
    "        \n",
    "        sns.set(font_scale=2)\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "        plt.xlabel('Предсказанные значения')\n",
    "        plt.xticks([0.5, 1.5, 2.5, 3.5], ['злость', 'радость', 'нейтральная', 'грусть'], rotation=45)\n",
    "        plt.yticks([0.5, 1.5, 2.5, 3.5], ['злость', 'радость', 'нейтральная', 'грусть'], rotation=45)\n",
    "        plt.ylabel('Верные значения')\n",
    "        plt.show()\n",
    "        \n",
    "    def save_combined_model(self, combined_model_output_file = 'combined_model'):\n",
    "        pickle.dump(self.combined_model, open(combined_model_output_file, 'wb'))\n",
    "        \n",
    "        \n",
    "    def load_combined_model(self, combined_model_output_file = 'combined_model'):\n",
    "        self.combined_model = pickle.load(open(combined_model_output_file, 'rb'))\n",
    "        \n",
    "clf = SpeechTextEmotionClassifier()\n",
    "\n",
    "text_df = clf.construct_text_df()\n",
    "speech_df = clf.construct_speech_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0444454",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_trainer = clf.create_ter_trainer(text_df, tp16=True, n_epochs=5, disable_tqdm=True)\n",
    "clf.train_model(text_trainer, save_model=True)\n",
    "speech_trainer = clf.create_ser_trainer(speech_df, tp16=True, n_epochs=8, disable_tqdm=False)\n",
    "clf.train_model(speech_trainer, save_model=False)\n",
    "\n",
    "clf.save_combined_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c615f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_trainer = clf.load_pretrained_model('ter', text_df, model_dir='ter_outputs/checkpoint-193', n_epochs=5, tp16=True, disable_tqdm=True)\n",
    "speech_trainer = clf.load_pretrained_model('ser', speech_df, model_dir='ser_outputs/checkpoint-1351', n_epochs=5, tp16=True, disable_tqdm=True)\n",
    "clf.load_combined_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7dc357",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.train_combined(text_trainer, speech_trainer, text_df['train'], speech_df['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8348cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def show_learning_plots(log_history):\n",
    "    metrics = pd.DataFrame(log_history)\n",
    "    metrics = metrics[['loss', 'eval_loss', 'eval_accuracy', 'epoch']]\n",
    "    metrics['train'] = metrics['eval_loss'].isna()\n",
    "    metrics['loss'].loc[metrics['loss'].isna()] = metrics[metrics['loss'].isna()]['eval_loss']\n",
    "    metrics['split'] = metrics['train'].apply(lambda x: 'train' if x else 'eval')\n",
    "    metrics.drop(['eval_loss', 'train'], axis=1, inplace=True)\n",
    "    sns.set_theme()\n",
    "    plt.rcParams.update({'font.size': 18, \"figure.figsize\": (20, 8)})\n",
    "    sns.set(font_scale=2)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "    sns.lineplot(data=metrics, x='epoch', y='loss', hue='split', legend='full', ax=ax1)\n",
    "    sns.lineplot(data=metrics, x='epoch', y='eval_accuracy', legend='full', ax=ax2)\n",
    "\n",
    "    \n",
    "def eval_metrics(trainer, ds):\n",
    "    logits = trainer.predict(ds)\n",
    "\n",
    "    predictions = np.argmax(logits.predictions, axis=1)\n",
    "    clf.metric.add_batch(predictions=predictions, references=ds[\"labels\"])\n",
    "\n",
    "    print(clf.metric.compute())\n",
    "    \n",
    "    \n",
    "    cm = confusion_matrix(ds[\"labels\"], predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['ang', 'hap', 'neu', 'sad'])\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title('Матрица ошибок')\n",
    "    sns.heatmap(cm,annot=True, fmt=\"d\")\n",
    "    \n",
    "    sns.set(font_scale=2)\n",
    "    plt.xlabel('Предсказанные значения')\n",
    "    plt.xticks([0.5, 1.5, 2.5, 3.5], ['злость', 'радость', 'нейтральная', 'грусть'], rotation=45)\n",
    "    plt.yticks([0.5, 1.5, 2.5, 3.5], ['злость', 'радость', 'нейтральная', 'грусть'], rotation=45)\n",
    "    plt.ylabel('Верные значения')\n",
    "    plt.show()\n",
    "        \n",
    "    \n",
    "    plt.rcParams.update({'font.size': 18, \"figure.figsize\": (14, 14)})\n",
    "\n",
    "    y_true = ds['labels']\n",
    "    y_pred = logits.predictions\n",
    "    skplt.metrics.plot_roc_curve(y_true, y_pred)\n",
    "    print('f1 score:')\n",
    "    for average in ['micro', 'macro', 'weighted', None]:\n",
    "        print(average, metrics.f1_score(y_true, predictions, average=average))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
